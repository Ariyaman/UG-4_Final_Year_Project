{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3NEbj_17Xx-"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUrKwA-l7ek8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI4Jh7Ea7nRU"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfVV7RsE7qXT"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s25t2ggu78CH"
      },
      "outputs": [],
      "source": [
        "# PlantVillage Dataset\n",
        "!kaggle datasets download -d emmarex/plantdisease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsh7V_C1D34T"
      },
      "outputs": [],
      "source": [
        "# Flavia Dataset\n",
        "!kaggle datasets download -d abdulhasibuddin/malayakew-plant-leaf-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyKr9F7y9d2P"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/plantdisease.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLYz5RHxbYzJ"
      },
      "outputs": [],
      "source": [
        "!rm -rf plantvillage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pX4g65HEBpu"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/malayakew-plant-leaf-dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1fAhvbQ8qlx"
      },
      "outputs": [],
      "source": [
        "mobileNet_V3_feat = 'https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5' #224\n",
        "efficientNet_V2_feat = 'https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2' #300\n",
        "cnn_model_one = mobileNet_V3_feat\n",
        "cnn_model_two = efficientNet_V2_feat\n",
        "model_name_one = \"MobileNet-V3-Large\"\n",
        "model_name_two = \"EfficientNet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFau3iCT8-bg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei-qRmWj11QL"
      },
      "outputs": [],
      "source": [
        "#used to regulate parameters for image resizing and tensorboard names and experiments\n",
        "batch_size = 64\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "transfer = \"Y\"\n",
        "dataset = \"PlantVillage\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfAELD9aJVtw"
      },
      "outputs": [],
      "source": [
        "cnn_model_layer_one = hub.KerasLayer(\n",
        "    handle=cnn_model_one,\n",
        "    input_shape=(img_height, img_width, 3),\n",
        "    trainable=False\n",
        ")\n",
        "cnn_model_layer_two = hub.KerasLayer(\n",
        "    handle=cnn_model_two,\n",
        "    input_shape=(img_height, img_width, 3),\n",
        "    trainable=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJGWE43pMUEs"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  str('/content/PlantVillage'),\n",
        "  validation_split=0.25,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  str('/content/PlantVillage'),\n",
        "  validation_split=0.25,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXzZg_QWMJWw"
      },
      "outputs": [],
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break\n",
        "\n",
        "train_ds_labels = np.array([])\n",
        "for x,y in train_ds:\n",
        "  train_ds_labels = np.concatenate([train_ds_labels, y.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdSZZKstLNIf"
      },
      "outputs": [],
      "source": [
        "feature_one_batch = cnn_model_layer_one(image_batch)\n",
        "print(feature_one_batch.shape)\n",
        "\n",
        "feature_two_batch = cnn_model_layer_two(image_batch)\n",
        "print(feature_two_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qXExaRvLTlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e7c26a-165c-4d8b-e997-9c7a86b948c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pepper__bell___Bacterial_spot' 'Pepper__bell___healthy'\n",
            " 'Potato___Early_blight' 'Potato___Late_blight' 'Potato___healthy'\n",
            " 'Tomato_Bacterial_spot' 'Tomato_Early_blight' 'Tomato_Late_blight'\n",
            " 'Tomato_Leaf_Mold' 'Tomato_Septoria_leaf_spot'\n",
            " 'Tomato_Spider_mites_Two_spotted_spider_mite' 'Tomato__Target_Spot'\n",
            " 'Tomato__Tomato_YellowLeaf__Curl_Virus' 'Tomato__Tomato_mosaic_virus'\n",
            " 'Tomato_healthy']\n",
            "15\n"
          ]
        }
      ],
      "source": [
        "class_names = np.array(train_ds.class_names)\n",
        "print(class_names)\n",
        "num_classes = len(class_names)\n",
        "print(num_classes)\n",
        "\n",
        "# Add normalization layer to reduce loss and improve accuracy\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# This causes instance to crash in COLAB due to overuse of RAM\n",
        "# AUTOTUNE = tf.data.AUTOTUNE\n",
        "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBqva5OiqUVk"
      },
      "outputs": [],
      "source": [
        "model_one = tf.keras.Sequential([\n",
        "  cnn_model_layer_one,\n",
        "  tf.keras.layers.Dense(num_classes, activation=tf.keras.activations.softmax)\n",
        "])\n",
        "\n",
        "model_two = tf.keras.Sequential([\n",
        "  cnn_model_layer_two,\n",
        "  tf.keras.layers.Dense(num_classes, activation=tf.keras.activations.softmax)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7YVM8dxLd5n"
      },
      "outputs": [],
      "source": [
        "prediction_one = model_one(image_batch)\n",
        "prediction_one.shape\n",
        "\n",
        "prediction_two = model_two(image_batch)\n",
        "prediction_two.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsSObyV_Lh1L"
      },
      "outputs": [],
      "source": [
        "model_one.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "  metrics=['acc'])\n",
        "\n",
        "model_two.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "  metrics=['acc'])\n",
        "\n",
        "curr_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "exp_name_one = model_name_one + \"_\"  + dataset + \"_\" + str(batch_size) + \"_\" + transfer\n",
        "log_dir_one = \"logs/fit/\" + model_name_one + \"/\" + curr_time + \"/\"\n",
        "\n",
        "exp_name_two = model_name_two + \"_\"  + dataset + \"_\" + str(batch_size) + \"_\" + transfer\n",
        "log_dir_two = \"logs/fit/\" + model_name_two + \"/\" + curr_time + \"/\"\n",
        "\n",
        "tensorboard_callback_one = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir_one,\n",
        "    histogram_freq=1\n",
        ") # Enable histogram computation for every epoch.\n",
        "\n",
        "tensorboard_callback_two = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir_two,\n",
        "    histogram_freq=1\n",
        ") # Enable histogram computation for every epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkDq479kLpC3"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, mode='max'), \n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='acc',\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "history_one = model_one.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=NUM_EPOCHS,\n",
        "  callbacks=[tensorboard_callback_one, callbacks]\n",
        ")\n",
        "\n",
        "history_two = model_two.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=NUM_EPOCHS,\n",
        "  callbacks=[tensorboard_callback_two, callbacks]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH4nN010ZsX0"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {log_dir_one} #MobileNetV3-Large model without FS logs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir {log_dir_two} #EfficientNetV2 model without FS logs"
      ],
      "metadata": {
        "id": "fgITt-DS8t4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transfer learning model\n",
        "feature_extractor_model_one = tf.keras.Sequential()\n",
        "feature_extractor_model_one.add(hub.KerasLayer(\n",
        "    handle=cnn_model_one,\n",
        "    input_shape=(img_height, img_width, 3),\n",
        "    trainable=False\n",
        "))\n",
        "\n",
        "feature_extractor_model_two = tf.keras.Sequential()\n",
        "feature_extractor_model_two.add(hub.KerasLayer(\n",
        "    handle=cnn_model_two,\n",
        "    input_shape=(img_height, img_width, 3),\n",
        "    trainable=False\n",
        "))\n",
        "\n",
        "feature_extractor_model_one.add(tf.keras.layers.Flatten())\n",
        "feature_extractor_model_two.add(tf.keras.layers.Flatten())\n",
        "\n",
        "feature_extractor_model_one.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "  metrics=['acc'])\n",
        "\n",
        "feature_extractor_model_two.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "  metrics=['acc'])"
      ],
      "metadata": {
        "id": "S3Wb84bKTeIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature vectors\n",
        "train_results_feature_vector_one = feature_extractor_model_one.predict(train_ds)\n",
        "train_results_feature_vector_two = feature_extractor_model_two.predict(train_ds)\n",
        "\n",
        "val_results_feature_vector_one = feature_extractor_model_one.predict(val_ds)\n",
        "val_results_feature_vector_two = feature_extractor_model_two.predict(val_ds)\n",
        "\n",
        "# Merging the feature vectors\n",
        "merged_train_feature_vector = keras.layers.Concatenate()([train_results_feature_vector_one, train_results_feature_vector_two])\n",
        "merged_val_feature_vector = keras.layers.Concatenate()([val_results_feature_vector_one, val_results_feature_vector_two])"
      ],
      "metadata": {
        "id": "Ntge-zePiyzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection"
      ],
      "metadata": {
        "id": "BiFXnYEVuIBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Py-FS"
      ],
      "metadata": {
        "id": "plZDEc_kHani"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid Feature Selection Algorithm\n",
        "from Py_FS.filter.Relief import Relief\n",
        "from Py_FS.wrapper.nature_inspired import GA\n",
        "\n",
        "\n",
        "relief_train_result = Relief(train_results_feature_vector, train_ds_labels).run()\n",
        "\n",
        "relief_train_result_ranks = relief_train_result.ranks()\n",
        "sorted_train_result_rank_subset = relief_train_result_ranks[:700]\n",
        "sorted_train_result_rank_subset_tensor = tf.convert_to_tensor(sorted_train_result_rank_subset, dtype=tf.int32)\n",
        "\n",
        "# This is the subset of the feature vector after ReliefF is used\n",
        "subset_train_feature_vector = tf.gather(merged_train_feature_vector, sorted_train_result_rank_subset_tensor, axis=1)\n",
        "\n",
        "ga_train_result = GA(10, 10, subset_train_feature_vector, train_ds_labels).run()\n",
        "final_train_feature_vector_indices = ga_train_result.Leader_agent[:300]\n",
        "final_train_feature_vector_indices_tensor = tf.convert_to_tensor(final_train_feature_vector_indices, dtype=tf.int32)\n",
        "\n",
        "# This is the subset of the feature vector after Genetic Algorithm is used\n",
        "final_train_subset_feature_vector = tf.gather(subset_train_feature_vector, final_train_feature_vector_indices_tensor, axis=1)\n",
        "\n",
        "\n",
        "relief_val_result = Relief(val_results_feature_vector, val_ds_labels).run()\n",
        "\n",
        "relief_val_result_ranks = relief_val_result.ranks()\n",
        "sorted_val_result_rank_subset = relief_val_result_ranks[:700]\n",
        "sorted_val_result_rank_subset_tensor = tf.convert_to_tensor(sorted_val_result_rank_subset, dtype=tf.int32)\n",
        "\n",
        "# This is the subset of the feature vector after ReliefF is used\n",
        "subset_val_feature_vector = tf.gather(merged_val_feature_vector, sorted_val_result_rank_subset_tensor, axis=1)\n",
        "\n",
        "ga_val_result = GA(10, 10, subset_val_feature_vector, val_ds_labels).run()\n",
        "final_val_feature_vector_indices = ga_val_result.Leader_agent[:300]\n",
        "final_val_feature_vector_indices_tensor = tf.convert_to_tensor(final_val_feature_vector_indices, dtype=tf.int32)\n",
        "\n",
        "# This is the subset of the feature vector after Genetic Algorithm is used\n",
        "final_val_subset_feature_vector = tf.gather(subset_val_feature_vector, final_val_feature_vector_indices_tensor, axis=1)"
      ],
      "metadata": {
        "id": "eIpe1bZJr64d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir_final = \"logs/fit/final/\" + curr_time + \"/\"\n",
        "final_model_after_fs = tf.keras.Sequential()\n",
        "final_model_after_fs.add(tf.keras.layers.Dense(num_classes, activation=tf.keras.activations.softmax))\n",
        "\n",
        "# Final model to get the dense layer extraction after the feature selection is done\n",
        "final_model_after_fs.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "  metrics=['acc'])\n",
        "\n",
        "exp_name_final = \"final\" + \"_\"  + dataset + \"_\" + str(batch_size)\n",
        "\n",
        "tensorboard_callback_final = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir_final,\n",
        "    histogram_freq=1\n",
        ") # Enable histogram computation for every epoch.\n",
        "\n",
        "history_one = final_model_after_fs.fit(\n",
        "  final_train_subset_feature_vector,\n",
        "  validation_data=final_val_subset_feature_vector,\n",
        "  epochs=NUM_EPOCHS,\n",
        "  callbacks=[tensorboard_callback_final, callbacks]\n",
        ")\n",
        "\n",
        "%tensorboard --logdir {log_dir_final} #Final model after feature selection logs"
      ],
      "metadata": {
        "id": "0VFhIHvdw13-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMu-H4JFsyEx"
      },
      "outputs": [],
      "source": [
        "!tensorboard dev upload \\\n",
        "  --logdir {log_dir_one} \\\n",
        "  --name {exp_name_one} \\\n",
        "  --one_shot\n",
        "\n",
        "!tensorboard dev upload \\\n",
        "  --logdir {log_dir_two} \\\n",
        "  --name {exp_name_two} \\\n",
        "  --one_shot\n",
        "\n",
        "!tensorboard dev upload \\\n",
        "  --logdir {log_dir_final} \\\n",
        "  --name {exp_name_final} \\\n",
        "  --one_shot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To do it for **Flavia dataset** just change the dataset that the directory is using to flavia\n",
        "## Change the img_height and img_width to 300 \n",
        "## Change the directory that train_ds and val_ds is refering to /content/MK/D2"
      ],
      "metadata": {
        "id": "pXKrvSxH2W1U"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}